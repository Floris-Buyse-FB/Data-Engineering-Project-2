{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250666, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data_clean/merged_total.csv')\n",
    "df.fillna('unknown', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\buyse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\buyse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['afspraak_keyphrases', 'account_keyphrases', 'campagne_keyphrases',\n",
       "       'sessie_keyphrases', 'visit_keyphrases', 'mailing_keyphrases'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../.env\")\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "cat_cols = cat_cols[(cat_cols != 'visit_bounce') & (cat_cols != 'contact_contactpersoon_id') & (cat_cols != 'account_account_id')]\n",
    "\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "    input=text,\n",
    "    model=embedding_model\n",
    "    )   \n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "\n",
    "def embed_col(df, col):\n",
    "    unique_col = df[col].unique().tolist()\n",
    "    dict_temp = {}\n",
    "\n",
    "    for i in unique_col:\n",
    "        dict_temp[i] = get_embedding(i)\n",
    "    \n",
    "    df[col+'_embed'] = df[col].map(dict_temp)\n",
    "    df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words_nl = set(stopwords.words('dutch'))\n",
    "    \n",
    "    word_tokens = word_tokenize(text, language='dutch')\n",
    "\n",
    "    result = [x for x in word_tokens if x not in stop_words_nl]\n",
    "\n",
    "    seperator = ', '\n",
    "    return seperator.join(result)\n",
    "\n",
    "\n",
    "def team_name_change(text):\n",
    "    teams_dict = {\n",
    "        'jo': ' jong ondernemen ',\n",
    "        'do': ' duurzaam ondernemen ',\n",
    "        'in': ' innovatie digitalisering ',\n",
    "        'io': ' internationaal ondernemen ',\n",
    "        'ao': ' arbeidsmarkt ',\n",
    "        'ex': ' expert ',\n",
    "        'gr': ' groei ',\n",
    "        'bb': ' belangenbehartiging ',\n",
    "        'co': ' communicatie ',\n",
    "        'nw': ' netwerking ',\n",
    "        'ha': ' haven ',\n",
    "        'ma': ' match '\n",
    "    }\n",
    "    word_tokens = word_tokenize(text, language='dutch')\n",
    "    # apply dict to list\n",
    "    result = [teams_dict.get(word, word) for word in word_tokens]\n",
    "    # join list to string\n",
    "    cleaned_list = ', '.join(result)\n",
    "    # tokenize string\n",
    "    tokenize_list = word_tokenize(cleaned_list, language='dutch')\n",
    "    # remove comma\n",
    "    tokenize_list_no_comma = [x for x in tokenize_list if x != ',']\n",
    "    # join list to string and remove duplicates from list\n",
    "    return ', '.join(list(set(tokenize_list_no_comma)))\n",
    "\n",
    "\n",
    "def clean_text(df, cat_cols=cat_cols):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    for col in cat_cols:\n",
    "        for row in range(len(df)):\n",
    "            name_change = team_name_change(df[col][row])\n",
    "            no_stopwords = remove_stopwords(name_change)\n",
    "            tokenize_list = word_tokenize(no_stopwords, language='dutch')\n",
    "            tokenize_list = [x for x in tokenize_list if x != ',']\n",
    "            df_copy.at[row, col] = ', '.join(list(set(tokenize_list)))\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def flatten_vector_columns(df, col):\n",
    "    # Flatten the 'vector_column' into a matrix (with padding)\n",
    "    max_vector_length = max(len(vector) for vector in df[col])\n",
    "    padded_matrix = np.array([vector + [0.0] * (max_vector_length - len(vector)) for vector in df[col]])\n",
    "    return padded_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duurt 11.5 minuten om df_clean te maken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = clean_text(df=df, cat_cols=cat_cols)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    df_clean[col] = df_clean[col].str.replace(r'\\d', '', regex=True).str.replace(', ,', ',')\n",
    "    df_clean[col] = df_clean[col].apply(lambda x: 'unknown' if len(x) == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oost, netwerking, regio, offline, vlaanderen, netwerkevenement, nieuwjaarsreceptie'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['campagne_keyphrases'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Campagne_keyphrases embeddings: 6 minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_emb1 = embed_col(df=df_clean, col='campagne_keyphrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_clean_emb1['campagne_keyphrases_embed'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Met OpenAI embedding -> 1536 getallen per keyphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the value from each embedded column to a single value\n",
    "def reduce_embedding(embedded):\n",
    "    return np.mean(embedded)\n",
    "\n",
    "df_clean['campagne_naam_embedded'] = df_clean['campagne_naam_embedded'].apply(lambda x: reduce_embedding(x))\n",
    "df_clean['visit_ip_embedded'] = df_clean['visit_ip_embedded'].apply(lambda x: reduce_embedding(x))\n",
    "df_clean['afspraak_keyphrases_embedded'] = df_clean['afspraak_keyphrases_embedded'].apply(lambda x: reduce_embedding(x))\n",
    "df_clean['mailing_name_embedded'] = df_clean['mailing_name_embedded'].apply(lambda x: reduce_embedding(x))\n",
    "df_clean['mailing_onderwerp_embedded'] = df_clean['mailing_onderwerp_embedded'].apply(lambda x: reduce_embedding(x))\n",
    "\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.5, min_samples=5, ).fit(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 60\n",
      "Estimated number of noise points: 9604\n"
     ]
    }
   ],
   "source": [
    "labels_db = dbscan.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "n_noise_ = list(labels_db).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'eps': 0.5,\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'euclidean',\n",
       " 'metric_params': None,\n",
       " 'min_samples': 5,\n",
       " 'n_jobs': None,\n",
       " 'p': None}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.get_params(deep=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
